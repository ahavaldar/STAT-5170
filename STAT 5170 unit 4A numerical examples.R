# =================================================================================
# STAT 5170: Applied Time Series
# Numerical examples for part A of learning unit 4
# =================================================================================
#
# In this set of examples, we will not be working with time series specifically, but will instead be setting up ideas that will help us makes inferences on time series data. The particular focus is on Bayesian methods of inference, which are introduced within some the standard contexts that a student would see in an introduction to statistics class: binomial experiments, i.i.d. normal samples, and linear regression.

# Some of the disucssion will involve concepts and computational techniques from probability and simulation. The numerical example from learning unit 1 goes over basic ideas in this direcitons, and would be worth reviewing as you go through the present set of examples.

# =================================================================================
# Analysis of binomial count data
# =================================================================================
#
# In the canonical setup for analysis of binomial count data, the data-generating distribution has x|theta ~ binomial(n, theta), the prior distribution has theta ~ beta(alpha, beta), and the posterior distribution has theta|x ~ beta(alpha+x, beta+n-x)

# The following code generate a graph of overlaid prior and posterior densities at specific values of alpha, beta, and x. The vertical dashed line markes the value x/n, the binomial proportion.

n <- 10
x <- 3
alpha <- 1/2
beta <- 1/2
x.grid <- seq(from=0, to=1, length.out=50)
prior.grid <- dbeta(x=x.grid, shape1=alpha, shape2=beta)
post.grid <- dbeta(x=x.grid, shape1=alpha+x, shape2=beta+n-x)

plot(x.grid, prior.grid, type="l", lty=1, lwd=3, col="green", xlim=c(0,1), ylim=c(0,max(post.grid)), xlab="", ylab="")
lines(x.grid, post.grid, lty=1, lwd=3, col="blue")
abline(h=0, lty=1, lwd=1)
abline(v=x/n, lty=2, lwd=2, col="black")

# The code below displays several numerical summaries of the posterior distribution. The posterior mean and posterior standard deviation of the binomial parameter, theta, are 

post.mean <- (alpha+x) / (alpha+beta+n)
post.mean

numer <- (alpha+x)*(beta+n-x)
denom <- (alpha+beta+n)^2*(alpha+beta+n+1)
post.sd <- sqrt(numer/denom)
post.sd

# Indications of these values are added to a plot of the posterior density generated by the following code.

yref <- max(post.grid)
plot(x.grid, post.grid, type="l", lty=1, lwd=3, col="blue", xlim=c(0,1), ylim=c(0,max(post.grid)), xlab="", ylab="")
lines((post.mean-post.sd)*c(1,1), yref*c(0.05,0.5), lty=1, lwd=3, col="red")
lines(post.mean*c(1,1), yref*c(0.05,0.5), lty=1, lwd=3, col="red")
lines((post.mean+post.sd)*c(1,1), yref*c(0.05,0.5), lty=1, lwd=3, col="red")
abline(h=0, lty=1, lwd=1)
abline(v=x/n, lty=2, lwd=2, col="black")

# The 2.5%, 25%, 50%, 75%, and 97.5% posterior quantiles are calculated and displayed as follows.

qprobs <- c(0.025, 0.25, 0.50, 0.75, 0.975)
post.quant <- qbeta(p=qprobs, shape1=alpha+x, shape2=beta+n-x)

n.qnt <- length(qprobs)
for (i.qnt in 1:n.qnt) {
	print(paste(sprintf("%4.1f", 100*qprobs[i.qnt]), "% quantile: ", sprintf("%6.4f", post.quant[i.qnt]), sep=""))
}

# These are displayed in a plot of the posterior density as follows.

yref <- max(post.grid)
plot(x.grid, post.grid, type="l", lty=1, lwd=3, col="blue", xlim=c(0,1), ylim=c(0,max(post.grid)), xlab="", ylab="")
for (i.qnt in 1:n.qnt) {
	lines(post.quant[i.qnt]*c(1,1), yref*c(0.05,0.5), lty=1, lwd=3, col="red")
}
abline(h=0, lty=1, lwd=1)
abline(v=x/n, lty=2, lwd=2, col="black")

# =================================================================================
# Analysis of independent and identically distributed normal data
# =================================================================================

# The next set of examples illustrates the use of Bayesian ideas for the analsis of independent and identically distributed normal data. Consider the following example data set of n=10 data points.

x.samp <- c(27.4, 43.6, 46.3, 46.3, 45.9, 67.9, 42.0, 80.0, 67.6, 57.4)
n <- length(x.samp)

# The sample mean, sample standard devation, and a histogram of these data is calculated as follows.

xbar <- mean(x.samp)
xbar

sdevx <- sd(x.samp)
sdevx

hist(x.samp, xlim=c(xbar-3*sdevx, xbar+3*sdevx), main="example data")

# A common setup for Bayesian analysis of an independent and identically distributed normal sample is to specify alongside the normal data-generating distribution xi ~ N(mu, sig2) a prior distribution factored into the conditional prior distribution mu|sig2 ~ N(m0, c0*sig2), and marginal prior distribution sig2 ~ ScInvChi2(lambda0, kappa0), where m0, c0, lambda0, and kappa0 are prior parameters to be specified with the model. The posterior distribution has mu|x,sig2 ~ N(m, c*sig2) and sig2|x ~ ScInvChi2(lambda, kappa), where m, c, lambda, and kappa are determined by the formulas below.

# ---------------------------------------------------------------------------------
# Marginal and conditional prior and posterior densities
# ---------------------------------------------------------------------------------

# The following code generate a graph of overlaid prior and posterior marginal densities of the variance parameter sig2 is generated at specific values of lambda0, and kappa0. 

# For this I first define a function for calculating the scaled inverse-chi-square density.

dscinvchisq <- function(x, scale, df) {
	val <- (0.5*scale)^(0.5*df)*x^(-(0.5*df+1))*exp(-0.5*scale/x)/gamma(0.5*df)
	return(val)
}

# The prior parameters are specified as

lambda0 <- 1
kappa0 <- 1

# The posterior parameters are calculated as

lambda <- lambda0 + (n-1)*sdevx^2
kappa <- kappa0 + (n-1)

# The plot is generated by the code below. The vertical dashed line marks the value of the sample variance.

xlim <- lambda / qchisq(p=c(0.98, 0.02), df=kappa)
x.grid <- seq(from=xlim[1], to=xlim[2], length.out=50)
prior.grid <- dscinvchisq(x=x.grid, scale=lambda0, df=kappa0)
post.grid <- dscinvchisq(x=x.grid, scale=lambda, df=kappa)

plot(x.grid, prior.grid, type="l", lty=1, lwd=3, col="green", xlim=xlim, ylim=c(0,max(post.grid)), xlab="", ylab="")
lines(x.grid, post.grid, lty=1, lwd=3, col="blue")
abline(h=0, lty=1, lwd=1)
abline(v=sdevx^2, lty=2, lwd=2, col="black")

# To explore the conditional posterior density of the mean parameter, mu, let us start by simulating a value of sig2 from its posterior distribution. 

sig2 <- lambda / rchisq(n=1, df=kappa)
sig2

# This value of sig2 is now fixed.

# The prior parameters are specified as

m0 <- 50
c0 <- 1.5

# The posterior parameters are calculated as

m <- (m0 + c0*n*xbar) / (1 + c0*n)
c <- c0 / (1 + c0*n)

# A plot of overlaid prior and posterior conditional densities of mu, given sig2, is generated at specific values of the prior parameters m0 and c0. The vertical dashed line marks the value of the sample mean.

xlim <- qnorm(p=c(0.02, 0.98), mean=m, sd=sqrt(c*sig2))
x.grid <- seq(from=xlim[1], to=xlim[2], length.out=50)
prior.grid <- dnorm(x=x.grid, mean=m0, sd=sqrt(c0*sig2))
post.grid <- dnorm(x=x.grid, mean=m, sd=sqrt(c*sig2))

plot(x.grid, prior.grid, type="l", lty=1, lwd=3, col="green", xlim=xlim, ylim=c(0,max(post.grid)), xlab="", ylab="")
lines(x.grid, post.grid, lty=1, lwd=3, col="blue")
abline(h=0, lty=1, lwd=1)
abline(v=xbar, lty=2, lwd=2, col="black")

# ---------------------------------------------------------------------------------
# Connections to classical inference
# ---------------------------------------------------------------------------------

# Setting the prior parameters to lambda0=0, kappa0=0 and c0 arbitrarily large (i.e. setting c0 to infinity) is sometimes said to configure the prior distribution for describing ignorance. In other words, it is sometimes said to set up a "non-informative" prior. Importantly, this setting also produces a posterior distribution that is configured for inferences that are in line with those of classical statistical inference. Specfically, the posterior distributon has
#
# mu|x,sig2 ~ N(xbar, s2/n)
#
# and
#
# sig2|x ~ ScInvChi2((n-1)*s2, n-1)
#
# The implication is that, unconditionally, the marginal posterior distribution of mu is such that
#
# t = (mu-xbar)/(s/sqrt(n))
#
# has a t distribution with df=n-1 degrees of freedom. It follows that, for 0 < alpha < 0.5, the 100*alpha/2 and 100*(1-alpha/2) posterior quantiles for mu are identical to the bounds on the classical 100*(1-alpha) confidence interval for mu
#
# xbar - tcrit*s/sqrt(n) < mu < xbar + tcrit*s/sqrt(n)
#
# where tcrit is the 1-alpha/2 quantile of a t distribution with df=n-1 degrees of freedom.
#
# The next set of examples illustrates how these pieces are put together.
#
# For the example data, let us first examine the marginal and conditional prior and posterior densities under the non-informative prior. 

# The marginal posterior density of sig2 is defined by the parameters...

lambda <- (n-1)*sdevx^2
kappa <- n-1

# The plot is generated as follows; the vertical dashed line locates the sample variance:

xlim <- lambda / qchisq(p=c(0.98, 0.02), df=kappa)
x.grid <- seq(from=xlim[1], to=xlim[2], length.out=50)
post.grid <- dscinvchisq(x=x.grid, scale=lambda, df=kappa)

plot(x.grid, post.grid, type="l", lty=1, lwd=3, col="blue", xlim=xlim, ylim=c(0,max(post.grid)), xlab="", ylab="")
abline(h=0, lty=1, lwd=1)
abline(v=sdevx^2, lty=2, lwd=2, col="black")

# The conditional posterior density of mu is specified at a simuated value of sig2:

sig2 <- lambda / rchisq(n=1, df=kappa)
sig2

# The conditional posterior density of mu  is defined by the parameters...

m <- xbar
c <- 1/n

# The plot is generated as follows; the vertical dashed line locates the sample mean:

xlim <- qnorm(p=c(0.02, 0.98), mean=m, sd=sqrt(c*sig2))
x.grid <- seq(from=xlim[1], to=xlim[2], length.out=50)
post.grid <- dnorm(x=x.grid, mean=m, sd=sqrt(c*sig2))

plot(x.grid, post.grid, type="l", lty=1, lwd=3, col="blue", xlim=xlim, ylim=c(0,max(post.grid)), xlab="", ylab="")
abline(h=0, lty=1, lwd=1)
abline(v=xbar, lty=2, lwd=2, col="black")

# Next let us take into account the uncertainty in sig2 by simulating multiple values of sig2 from its posterior distribution, and for each one graphing the corresponding posterior conditional density of mu, given sig2. The following code generates a plot with these densities overlaid.

n.graph <- 10
xlim <- qnorm(p=c(0.02, 0.98), mean=m, sd=sqrt(c*sig2))
x.grid <- seq(from=xlim[1], to=xlim[2], length.out=50)
post.grid <- matrix(data=0, nrow=n.graph, ncol=length(x.grid))
for (i.graph in 1:n.graph) {
	sig2 <- lambda / rchisq(n=1, df=kappa)
	post.grid[i.graph, ] <- dnorm(x=x.grid, mean=m, sd=sqrt(c*sig2))
}
plot(x.grid[1], 0, type="l", lty=1, lwd=3, col="white", xlim=xlim, ylim=c(0,max(post.grid)), xlab="", ylab="")
for (i.graph in 1:n.graph) {
	lines(x.grid, post.grid[i.graph, ], lty=1, lwd=1, col="blue")
}
abline(h=0, lty=1, lwd=1)
abline(v=xbar, lty=2, lwd=2, col="black")

# Such a plot does not fully fold in the uncertainty in sig2, but it does give us a sense of its impact.

# Consider that a statistical report would not be presented in terms of densities, but in terms of posterior summaries such as mean, standard deviations, and quantiles. The exercise of depicting the impact of uncertainty in sig2 by plotting densities gives us a sense of how to fully incorporate uncertainty in sig2 into posterior summary statistics. The solution it suggests is to repeatedly simulate sig2 from its posterior distribution (creating a sample from its posterior distribution), such that at each repetition the desired posterior summary from the posterior conditional distribution of mu given sig2 is calculated; when a sufficient number of such summaries are generated, the are averaged together, and that average is reported for the posterior summary.

# This is demonstrated as follows, for each of the following posterior summaries for mu: the mean, standard deviation, and several posterior quantiles.

n.samp <- 10000
qprobs <- c(0.025, 0.25, 0.5, 0.75, 0.975)
post.quant <- matrix(data=0, nrow=length(qprobs), ncol=n.samp)
post.sig2 <- numeric(length=n.samp)
post.mean <- numeric(length=n.samp)
post.sdev <- numeric(length=n.samp)
for (i.samp in 1:n.samp) {
	sig2 <- lambda / rchisq(n=1, df=kappa)
	post.sig2[i.samp] <- sig2
	post.mean[i.samp] <- m
	post.sdev[i.samp] <- sqrt(c*sig2)
	post.quant[,i.samp] <- qnorm(p=qprobs, mean=m, sd=sqrt(c*sig2))
}

# As a starting point to examining the output to the code above, consider the first few simulated values of sig2 and the corresponding values of each posterior summary for mu:

for (i.samp in 1:3) {
	print(paste("*** Sample value ", i.samp, ": ***", sep=""))
	print(paste("sig2: ", sprintf("%6.2f", post.sig2[i.samp]), ":", sep=""))
	print(paste("mean: ", sprintf("%6.2f", post.mean[i.samp]), ":", sep=""))
	print(paste("sdev: ", sprintf("%6.2f", post.sdev[i.samp]), ":", sep=""))
	for (i.qnt in 1:length(qprobs)) {
		print(paste(sprintf("%4.1f", 100*qprobs[i.qnt]), "% quantile:", sprintf("%6.2f", post.quant[i.qnt, i.samp]), ":", sep=""))
	}
	print("")
}

# Observe that every posterior summary, except the posterior mean, varies with sig2. This is to be expected, since we are working with the conditional posterior distribution for mu, given sig2.

# A statistical report would fold in the uncertainty about sig2 by averaging the values of each statistic across all simulated sig2:

print(paste("mean: ", sprintf("%5.2f", mean(post.mean)), sep=""))
print(paste("sdev: ", sprintf("%5.2f", mean(post.sdev)), sep=""))
for (i.qnt in 1:length(qprobs)) {
	print(paste(sprintf("%4.1f", 100*qprobs[i.qnt]), "% quantile:", sprintf("%6.2f", mean(post.quant[i.qnt,])), sep=""))
}

# Let us now compare these values with the corresponding summary statistics that are defined in classical inference.

# The two point estimates for mu are...

Bayes.stat <- mean(post.mean)
Class.stat <- xbar
print(paste("Bayes: ", sprintf("%5.2f", mean(Bayes.stat)), "; Classical: ", sprintf("%5.2f", mean(Class.stat)), sep=""))

# We see that these two calculated values are VERY close.

# The two point estimates for sig are...

Bayes.stat <- mean(sqrt(post.sig2))
Class.stat <- sdevx
print(paste("Bayes: ", sprintf("%5.2f", mean(Bayes.stat)), "; Classical: ", sprintf("%5.2f", mean(Class.stat)), sep=""))

# These values are close, but not quite the same. The same is true for summary statistics that would provide standard errors for mu. These are...

Bayes.stat <- mean(post.sdev)
Class.stat <- sdevx/sqrt(n)
print(paste("Bayes: ", sprintf("%5.2f", mean(Bayes.stat)), "; Classical: ", sprintf("%5.2f", mean(Class.stat)), sep=""))

# The quantiles and confidence interval bounds are...

for (i.qnt in 1:length(qprobs)) {
	Bayes.stat <- mean(post.quant[i.qnt,])
	Class.stat <- xbar + qt(p=qprobs[i.qnt], df=n-1)*sdevx/sqrt(n)
	print(paste("*** ", sprintf("%4.1f", 100*qprobs[i.qnt]), "% quantile: ***", sep=""))
	print(paste("Bayes: ", sprintf("%5.2f", mean(Bayes.stat)), "; Classical: ", sprintf("%5.2f", mean(Class.stat)), sep=""))
	print("")
}

# Again, we see that these two calculated values at any particaular quantile value are very close.

# The results examined above reflect that when we fold in uncertainty about sig2 into inferences about mu, we end up working with a t distribution. That is, the marginal posterior distribution of mu, under the non-informative prior, is a t distrution. However, when calculating the Bayesian posterior quanties for mu, we never simulated a value of mu from its marginal posterior t distribution; we instead implemented the two-step strategy of first simulating sig2 (from its marginal posterior SclInvChiSq distribution), and then, given sig2, simulated mu (from its conditional posterior normal distribution). This is known as a "Gibbs sampling" strategy.

# =================================================================================
# Linear regression
# =================================================================================

# In the next set of examples, we apply regression to Yule's "Marriages in the Church of England," in an effort to "detrend" it. To refresh our memory of this data set, the following code reads in the data and generates a plot

setwd(DataDirectory)
marriages.raw <- scan(file="marriages.txt", sep="\n")
n <- length(marriages.raw)

marriages.ts <- ts(marriages.raw, frequency=1, start=1866)
ts.plot(marriages.ts, xlab="year", ylab="percent", main="Marriages in the Church of England")

# Here, regression is used to develop a model that would describe the downward trend in the data. This trend is formulated as a linear function of the time series's mean function, mut = E[xt], against time:
# 
# mut = beta1 + beta2*t
#
# in which we expect the beta2 coefficient to be negative.
#
# In a preliminary version of the model, the time series itself is understood as this decreasing linear function, plus residual error, which is taken to be white noise with standard deviation sigw. Thus,
# 
# xt = beta1 + beta2*t + wt
#
# A vector-matrix formula of this model defines a response vector and regression matrix as follows:

x.vect <- as.matrix(marriages.ts)
Z.mat <- as.matrix(cbind(rep(x=1, times=n), 1866:1911))

# The regression model may then be expressed as
#
# x.vect | beta, sigw ~ N(Z.mat*beta.vect, sigw2*I)
#
# where beta = [beta1, beta2]^T is the vector containing the regression coefficients, and I denotes the identity matrix. It would not be unreasonable that the covariance matrix in this model should instead take into account autocorrelation; in the analysis below we will check the whether a model that does take autocorrelation into account is more suited to these data.

# To implement Bayesian inference, we adopt a non-informative prior, so that the posterior distribution is easily formulated from statistics that would be calculated in least-squares analysis. For convenience, the calculations that produce the latter statistics are collected into a user-defined function below. The function also calculates additional quantities that will be used when simulating values from the posterior distribution. One of these calculations is a matrix square-root, which requires a function from an external package that must be loaded into the R session:

#install.packages("expm")
library(expm)

# The user-defined function is created as follows. 

reg.summary <- function(x.vect, Z.mat) {
	n <- dim(Z.mat)[1]
	k <- dim(Z.mat)[2]
	nu <- n-k
	ZpZ <- t(Z.mat) %*% Z.mat
	ZpX <- t(Z.mat) %*% x.vect
	ZpZ.inv <- solve(ZpZ)
	sqrt.ZpZ.inv <- sqrtm(ZpZ.inv) # The sqrtm() function is matrix square-root 
	b.hat <- ZpZ.inv %*% ZpX
	res <- as.vector(x.vect - Z.mat %*% b.hat)
	s2 <- sum(res^2) / nu
	out.list <- list(sqrt.ZpZ.inv=sqrt.ZpZ.inv, b.hat=b.hat, s2=s2, nu=nu, res=res)
	return(out.list)
}

# For these data, the least-squares regression coefficients are calculated as...

result <- reg.summary(x.vect, Z.mat)
b.hat <- result$b.hat
b.hat

# The regression line defined by these values is added into the time series plot as follows

ts.plot(marriages.ts, xlab="year", ylab="percent", main="Marriages in the Church of England")
abline(a=b.hat[1], b=b.hat[2], lty=1, lwd=3, col="blue")

# Simulating from the posterior distribution allows us to calculate quantiles of each mean-function value, mut = beta1 + beta2*t. This is implemented as follows.

n.samp <- 10000
n <- dim(Z.mat)[1]
k <- dim(Z.mat)[2]
result <- reg.summary(x.vect, Z.mat)
sqrt.ZpZ.inv <- result$sqrt.ZpZ.inv
b.hat <- result$b.hat
s2 <- result$s2
nu <- result$nu
logsigw2.samp <- numeric(length=n.samp)
beta.samp <- matrix(nrow=k, ncol=n.samp)
mu.samp <- matrix(nrow=n, ncol=n.samp)
for (i.samp in 1:n.samp) {
	sigw2 <- nu*s2 / rchisq(n=1, df=nu)
	sigw <- sqrt(sigw2)
	beta <- b.hat + sqrt.ZpZ.inv %*% as.matrix(rnorm(n=k, mean=0, sd=sigw))
	logsigw2.samp[i.samp] <- log(sigw2)
	beta.samp[, i.samp] <- as.vector(beta)
	mu.samp[, i.samp] <- as.vector(Z.mat %*% beta)
}

# Quantiles of the basic regression parameters are displayed as follows

qprobs <- c(0.025, 0.25, 0.5, 0.75, 0.975)
beta1 <- quantile(beta.samp[1,], probs=qprobs)
beta2 <- quantile(beta.samp[2,], probs=qprobs)
sigw <- quantile(exp(0.5*logsigw2.samp), probs=qprobs)
rbind(beta1, beta2, sigw)

# Quantiles of the mean-function values, mut, are incorporated into the time series plot as follows

ts.plot(marriages.ts, xlab="year", ylab="percent", main="Marriages in the Church of England")
qprobs <- c(0.025, 0.5, 0.975)
for (i.tval in 1:n) {
	t <- Z.mat[i.tval, 2]
	qstat <- as.numeric(quantile(mu.samp[i.tval,], probs=qprobs))
	lines(t*c(1, 1), c(qstat[1], qstat[3]), type="l", lty=1, lwd=1, col="blue")
	points(t, qstat[2], pch=19, cex=0.5, col="blue")
}

# The quantiles that appear in this plot are to be compared with what are sometimes called "confidence bands" in classical regression.

# We might also contemplate the possiblility to go back in time, to 1865, and use the data that we have now to predict the values in a regeneration of this time series. For this purpose, we would work with the posterior predictive distribution. Simulated values from this distribution are generated as follows.

n.samp <- 10000
x.regen.samp <- matrix(nrow=n, ncol=n.samp)
for (i.samp in 1:n.samp) {
	sigw2 <- nu*s2 / rchisq(n=1, df=nu)
	sigw <- sqrt(sigw2)
	beta <- b.hat + sqrt.ZpZ.inv %*% as.matrix(rnorm(n=k, mean=0, sd=sigw))
	mu.vect <- Z.mat %*% beta
	x.regen <- mu.vect + as.matrix(rnorm(n=n, mean=0, sd=sigw))
	x.regen.samp[, i.samp] <- as.vector(x.regen)
}

# Notice in each iteration of this simulation that once a full set of parameters is simulated from the posterior distribution, the full time series itself is simulated from the data-generating distribution.

# Quantiles of the regenerated time-series values are incorporated into the time series plot as follows

ts.plot(marriages.ts, xlab="year", ylab="percent", main="Marriages in the Church of England")
qprobs <- c(0.025, 0.5, 0.975)
for (i.tval in 1:n) {
	t <- Z.mat[i.tval, 2]
	qstat <- as.numeric(quantile(x.regen.samp[i.tval,], probs=qprobs))
	lines(t*c(1, 1), c(qstat[1], qstat[3]), type="l", lty=1, lwd=1, col="blue")
	points(t, qstat[2], pch=19, cex=0.5, col="blue")
}

# The quantiles that appear in this plot are to be compared with what are sometimes called "prediction bands" in classical regression.

# ---------------------------------------------------------------------------------
# Model checking
# ---------------------------------------------------------------------------------
#
# The technique of simulating regenerated time series from the posterior predictive distribution gives rise to the possibility of checking whether the standard linear regression model (with its assumption of white-noise as residual errors) adequately accounts for patterns observed in the data at hand.
#
# We can check the white-noise assumption by calculating sample autocorrelations on the residual errors of each regenerated time series, and compare them to the same sample autocorrelations calculated on the residual errors of the data at hand. These simualated sample autocorrelations are generated by the code below, which are calculated with the help of a user-defined function.

# The user-defined, which calculates the autocorrelations in a vector of residuals, is...

res.acf <- function(res, sigw2, max.lag) {
	n <- dim(res)[1]
	acf.seq <- numeric(length=max.lag)
	for (lag in 1:max.lag) {
		acf.seq[lag] <- sum(res[(1+lag):n]*res[1:(n-lag)])/((n-lag)*sigw2)
	}
	return(acf.seq)
}

# The simulation is carried out as follows.

n.samp <- 10000
max.lag <- 5
acf.samp <- matrix(nrow=max.lag, ncol=n.samp)
acf.regen.samp <- matrix(nrow=max.lag, ncol=n.samp)
for (i.samp in 1:n.samp) {
	sigw2 <- nu*s2 / rchisq(n=1, df=nu)
	sigw <- sqrt(sigw2)
	beta <- b.hat + sqrt.ZpZ.inv %*% as.matrix(rnorm(n=k, mean=0, sd=sigw))
	mu.vect <- Z.mat %*% beta
	res <- x.vect - mu.vect
	x.regen <- mu.vect + as.matrix(rnorm(n=n, mean=0, sd=sigw))
	res.regen <- x.regen - mu.vect
	acf.samp[, i.samp] <- res.acf(res, sigw2, max.lag)
	acf.regen.samp[, i.samp] <- res.acf(res.regen, sigw2, max.lag)
}

# Check whether the autocorrelations in the observed data are not suitably accounted for in the model, compare the relative frequency at which the magnitude of each autocorrelation in the regenerated data exceeds that of the corresponding autocorrelation of the measured data. The resulting relative frequency is a posterior predictive p-value. These values are calculated at each lag by the following code.

for (lag in 1:max.lag) {
	pval <- length(which(abs(acf.regen.samp[lag,]) > abs(acf.samp[lag,]))) / n.samp
	print(paste("lag ", lag, " p-value: ", sprintf("%6.4f", pval), sep=""))
}

# A posterior predictive p-value that is close to 0 or 1 would indicate that the model does not suitably account for the patterns summarized by the autocorrelation statistic. In the results produced by the previous code, we see that lag-1 autocorrelations may not be suitably taken into account.

# It is sometimes worthwhile to plot the simulated autocorrelation values from the regenerated and measured data as a scatterplot. Such a plot is generated for lag-1 autocorrelations by the following code.

lag <- 1
pval <- length(which(abs(acf.regen.samp[lag,]) > abs(acf.samp[lag,]))) / n.samp
plot(acf.samp[lag,], acf.regen.samp[lag,], pch=16, cex=1, xlim=c(-1, 1), ylim=c(-1, 1))
abline(a=0, b=1)
text(x=-0.8, y=0.8, labels=paste("pval =", sprintf("%4.2f", pval)), pos=4)

# A cloud of values falling well below or well above the 45 degree line corresponds to a p-value close to 0 or 1.

# ---------------------------------------------------------------------------------
# Aggregated statistics
# ---------------------------------------------------------------------------------

# Rather than checking autocorrelations at individual lags, it may also be helpful to aggregate the autocorrelations across multiple lags. There is no one way to do this, but two potential approaches are explored below. The first is the maximum absolute autocorrelation up to a given lag, and the second is the sum of absolute autocorrelations up to a given lag.

# Each of these approaches to aggregating autocorrelation is use to calculate a posterior predictive p-value as follows.

max.acf.samp <- numeric(length=n.samp)
sum.acf.samp <- numeric(length=n.samp)
max.acf.regen.samp <- numeric(length=n.samp)
sum.acf.regen.samp <- numeric(length=n.samp)
for (i.samp in 1:n.samp) {
	max.acf.samp[i.samp] <- max(abs(acf.samp[,i.samp]))
	sum.acf.samp[i.samp] <- sum(abs(acf.samp[,i.samp]))
	max.acf.regen.samp[i.samp] <- max(abs(acf.regen.samp[,i.samp]))
	sum.acf.regen.samp[i.samp] <- sum(abs(acf.regen.samp[,i.samp]))
}

# The posterior predictive p-value and associated scatterplot defined from the maximum of absolute autocorrelations are generated as follows

pval <- length(which(max.acf.regen.samp > max.acf.samp)) / n.samp
pval

plot(max.acf.samp, max.acf.regen.samp, pch=16, cex=1, xlim=c(-1, 1), ylim=c(-1, 1))
abline(a=0, b=1)
text(x=-0.8, y=0.8, labels=paste("pval =", sprintf("%4.2f", pval)), pos=4)

# The posterior predictive p-value and associated scatterplot defined from the sum of absolute autocorrelations are generated as follows

pval <- length(which(sum.acf.regen.samp > sum.acf.samp)) / n.samp
pval

plot(sum.acf.samp, sum.acf.regen.samp, pch=16, cex=1, xlim=c(0, 3), ylim=c(0, 3))
abline(a=0, b=1)
text(x=0.1, y=2.5, labels=paste("pval =", sprintf("%4.2f", pval)), pos=4)

# In this example, the posterior predictive p-value calculated on the individual lag-1 autocorrelation is most discerning, but there may be instances when the aggregates statistics tell us more. In addition, a variation of the sum of absolute autocorrelations statistic, in which the lags are weighted, will later make a connection to model checking statistics that are sometimes used in classical model checking. 
